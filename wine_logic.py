import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import streamlit as st

load_dotenv()

@st.cache_resource
def get_wine_rag_chain():
    # 1. ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    # 2. ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
    vectorstore = PineconeVectorStore(
        index_name="wine-sommelier-agent", 
        embedding=embeddings
    )

    # 3. ì…€í”„ ì¿¼ë§ ì„¤ì •
    metadata_field_info = [
        AttributeInfo(
            name="country",
            description="The country where the wine was produced. For European wines, use: France, Italy, Spain, Portugal, Germany, Austria, Greece. For American wines, use: US. For Oceania: Australia, New Zealand. For South American wines: Chile, Argentina.",
            type="string"
        ),
        AttributeInfo(name="points", description="Wine rating score from 80 to 100", type="integer"),
        AttributeInfo(name="price", description="Price in USD", type="float"),
        AttributeInfo(name="variety", description="Grape variety like Chardonnay, Pinot Noir, Cabernet Sauvignon", type="string"),
        AttributeInfo(name="tag_oak", description="1 if wine has oak aging flavor, else 0", type="integer"),
    ]
    document_content_description = "Wine tasting notes, flavor profile, and characteristics description"

    retriever = SelfQueryRetriever.from_llm(
        llm, vectorstore, document_content_description, metadata_field_info, verbose=True
    )

    # 4. RAG í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸
    template = """ë‹¹ì‹ ì€ ë°ì´í„° ê¸°ë°˜ ì „ë¬¸ ì†Œë¯ˆë¦¬ì—ì…ë‹ˆë‹¤.
    ë°˜ë“œì‹œ ì•„ë˜ì˜ [ì¶œë ¥ ì–‘ì‹]ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

    **ì¤‘ìš”í•œ ê·œì¹™:**
    1. ì™€ì¸ ì´ë¦„ì€ **ë°˜ë“œì‹œ ì˜ì–´ ì›ë¬¸**ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš” (ì˜ˆ: ChÃ¢teau Margaux, not ìƒ¤í†  ë§ˆê³ )
    2. ì „ë¬¸ê°€ ë…¸íŠ¸, ì„ ì • ì´ìœ , í˜ì–´ë§ ì œì•ˆ ë“± **ëª¨ë“  ì„¤ëª…ì€ í•œê¸€**ë¡œ ì‘ì„±í•˜ì„¸ìš”
    3. ì‚¬ìš©ìê°€ "ìœ ëŸ½ ì™€ì¸"ì„ ìš”ì²­í•˜ë©´ France, Italy, Spain, Portugal, Germany ë“±ì˜ ìœ ëŸ½ êµ­ê°€ë§Œ ì„ íƒí•˜ì„¸ìš”
    4. ì‚¬ìš©ìê°€ "ë¯¸êµ­ ì™€ì¸"ì„ ìš”ì²­í•˜ë©´ ë°˜ë“œì‹œ countryê°€ "US"ì¸ ì™€ì¸ë§Œ ì„ íƒí•˜ì„¸ìš”
    5. ì‚¬ìš©ìê°€ íŠ¹ì • ëŒ€ë¥™/ì§€ì—­ì„ ì–¸ê¸‰í•˜ë©´ í•´ë‹¹ ì§€ì—­ì˜ ì™€ì¸ë§Œ ì¶”ì²œí•˜ì„¸ìš”

    ### [ì¶œë ¥ ì–‘ì‹]
    ---
    #### ğŸ· ì¶”ì²œ ì™€ì¸: [ì™€ì¸ ì˜ì–´ ì´ë¦„ ë° ë¹ˆí‹°ì§€]
    - **ì‚°ì§€/í’ˆì¢…:** [êµ­ê°€ëª…] | [í¬ë„ í’ˆì¢…]
    - **ë°ì´í„° ë¶„ì„:** í‰ì  **[XXì ]** / ê°€ê²© **$[XX]** (ê°€ì„±ë¹„ ì§€ìˆ˜: [ìš°ìˆ˜/ë³´í†µ])
    - **ì „ë¬¸ê°€ ë…¸íŠ¸:** [ë§›ê³¼ í–¥ì— ëŒ€í•œ í•µì‹¬ ì„¤ëª… 2ë¬¸ì¥ ì´ë‚´ - ë°˜ë“œì‹œ í•œê¸€ë¡œ]
    - **ì„ ì • ì´ìœ :** [ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ ì—°ê³„ëœ ë…¼ë¦¬ì  ì¶”ì²œ ê·¼ê±° - ë°˜ë“œì‹œ í•œê¸€ë¡œ]
    - **ğŸ´ í˜ì–´ë§ ì œì•ˆ:** [ì–´ìš¸ë¦¬ëŠ” ìŒì‹ 1~2ê°€ì§€ - ë°˜ë“œì‹œ í•œê¸€ë¡œ]
    ---

    ê²€ìƒ‰ëœ ì™€ì¸ ì •ë³´:
    {context}

    ì‚¬ìš©ì ì§ˆë¬¸: {question}

    ë‹µë³€:"""
    prompt = ChatPromptTemplate.from_template(template)

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain